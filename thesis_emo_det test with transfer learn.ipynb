{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of thesis_emo_det test 2.ipynb","provenance":[{"file_id":"1kCj7lWZFhE7TiTg_rIF9gmLXxM7DjBu2","timestamp":1581966735098}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1-Rwvz4LLmld02H2xA_KUVZabaLcO_LUL","authorship_tag":"ABX9TyNij8sfzFpZ3bFiwAqqdj8x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"bTxbi6_oWSu5","colab_type":"code","outputId":"275898a6-55dc-4d01-f47c-35afa0874863","executionInfo":{"status":"ok","timestamp":1582008317369,"user_tz":-360,"elapsed":2952,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["\n","import os\n","import zipfile\n","import shutil\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import tensorflow as tf\n","import cv2\n","import matplotlib.pyplot as plt\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"SPo2mjdvTA8S","colab_type":"code","colab":{}},"source":["local_zip = '/content/drive/My Drive/Thesis/KDEF_and_AKDEF.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","try:\n","    os.makedirs('/content/Thesis_Emodet', exist_ok=True)\n","except OSError as error:\n","    print(error)\n","zip_ref.extractall('/content/Thesis_Emodet')\n","zip_ref.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"moUbuTLcTPr4","colab_type":"code","colab":{}},"source":["files = []\n","dir = []\n","basepath = '/content/Thesis_Emodet/KDEF_and_AKDEF/KDEF'\n","for entry in os.listdir(basepath):\n","    sub=[]\n","    dir.append(basepath+'/'+entry+'/')\n","    for file in os.listdir(basepath+'/'+entry):\n","        sub.append(file)\n","    files.append(sub)\n","\n","root='/content/Thesis_Emodet/emotions/'\n","emotion_type=['AF','AN','DI','HA','NE','SA','SU']\n","\n","for sublist in range(len(files)):\n","    for file in files[sublist]:\n","        for i in range (len(emotion_type)):\n","            if file[4:6] == emotion_type[i]:\n","                try:\n","                    os.makedirs(root+emotion_type[i], exist_ok=True)\n","                except OSError as error:\n","                    print(error)\n","                shutil.copy(dir[sublist]+file, root+emotion_type[i])\n","                break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OarKYk1zhklb","colab_type":"code","colab":{}},"source":["basepath2 = '/content/Thesis_Emodet/KDEF_and_AKDEF/AKDEF'\n","files_AKDEF = []\n","for entry in os.listdir(basepath2):\n","    files_AKDEF.append(entry)\n","for file in files_AKDEF:\n","    for i in range (len(emotion_type)):\n","        if file[1:3] == emotion_type[i]:\n","            try:\n","                os.makedirs(root+emotion_type[i], exist_ok=True)\n","            except OSError as error:\n","                print(error)\n","                #print(root+emotion_type[i]+'/'+file)\n","            shutil.copy(basepath2+'/'+file, root+emotion_type[i])\n","            break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"je3fuJjaQ6ah","colab_type":"code","colab":{}},"source":["# Call_back function for stopping training after a certain level\n","\n","class myCallback(tf.keras.callbacks.Callback):# Your Code\n","    def on_epoch_end(self, epoch, logs={}):\n","        if(logs.get('acc')>.95):\n","            print(\"\\nReached 95% accuracy so cancelling training!\")\n","            self.model.stop_training = True\n","\n","AccuracyLimiterCallback = myCallback()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FJp5Z4Z6YIC","colab_type":"code","colab":{}},"source":["checkpoint_path = \"/content/drive/My Drive/Thesis/test_2 cp/cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)\n","\n","\n","\n","# This may generate warnings related to saving the state of the optimizer.\n","# These warnings (and similar warnings throughout this notebook)\n","# are in place to discourage outdated usage, and can be ignored."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOMmwhVWmL1-","colab_type":"code","outputId":"7c7971b5-4e2b-4bb2-a54e-5eebcec5c30a","executionInfo":{"status":"ok","timestamp":1582008332144,"user_tz":-360,"elapsed":17631,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# For data split into test & validation\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    rotation_range=60,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    fill_mode='nearest',\n","    validation_split=0.15) # set validation split\n","\n","batch_size=32\n","img_height=572\n","img_width=422\n","train_data_dir='/content/Thesis_Emodet/emotions/' # source directory for training images\n","train_generator = train_datagen.flow_from_directory(\n","    directory=train_data_dir,\n","    #color_mode='grayscale',\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training') # training data\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    directory=train_data_dir, # same directory as training data\n","    #color_mode='grayscale',\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation') # validation data\n","\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Found 4225 images belonging to 7 classes.\n","Found 742 images belonging to 7 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1xJZ5glPPCRz","outputId":"119b5fc4-ae4e-40ad-deee-cf56cd390863","executionInfo":{"status":"ok","timestamp":1582008349318,"user_tz":-360,"elapsed":34783,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n","    -O /content/Thesis_Emodet/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","  \n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","\n","local_weights_file = '/content/Thesis_Emodet/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","\n","pre_trained_model = InceptionV3(input_shape = (572 , 422, 3), \n","                                include_top = False, \n","                                weights = None)\n","\n","pre_trained_model.load_weights(local_weights_file)\n","\n","for layer in pre_trained_model.layers:\n","  layer.trainable = False\n","\n","#pre_trained_model.summary()\n","\n","last_layer = pre_trained_model.get_layer('mixed8')\n","print('last layer output shape: ', last_layer.output_shape)\n","last_output = last_layer.output"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2020-02-18 06:45:32--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.12.128, 2607:f8b0:400c:c13::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.12.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87910968 (84M) [application/x-hdf]\n","Saving to: ‘/content/Thesis_Emodet/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n","\n","\r          /content/   0%[                    ]       0  --.-KB/s               \r         /content/T  40%[=======>            ]  34.37M   172MB/s               \r        /content/Th  95%[==================> ]  80.23M   201MB/s               \r/content/Thesis_Emo 100%[===================>]  83.84M   204MB/s    in 0.4s    \n","\n","2020-02-18 06:45:32 (204 MB/s) - ‘/content/Thesis_Emodet/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","last layer output shape:  (None, 16, 11, 1280)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BMXb913pbvFg","outputId":"4c9af3a6-ae37-41d5-b145-6ae29e9bf6f9","executionInfo":{"status":"ok","timestamp":1582008349320,"user_tz":-360,"elapsed":34751,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","# Flatten the output layer to 1 dimension\n","x = layers.Flatten()(last_output)\n","# Add a fully connected layer with 1,024 hidden units and ReLU activation\n","x = layers.Dense(1024, activation='relu')(x)\n","# Add a dropout rate of 0.2\n","x = layers.Dropout(0.4)(x)                  \n","# Add a final sigmoid layer for classification\n","x = layers.Dense  (7, activation='softmax')(x)           \n","\n","model = Model( pre_trained_model.input, x) \n","\n","model.compile(optimizer = RMSprop(lr=0.0001), \n","              loss = 'categorical_crossentropy', \n","              metrics = ['acc'])\n","model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 572, 422, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 285, 210, 32) 864         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 285, 210, 32) 96          conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 285, 210, 32) 0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 283, 208, 32) 9216        activation[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 283, 208, 32) 96          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 283, 208, 32) 0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 283, 208, 64) 18432       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 283, 208, 64) 192         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 283, 208, 64) 0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 141, 103, 64) 0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 141, 103, 80) 5120        max_pooling2d[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 141, 103, 80) 240         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 141, 103, 80) 0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 139, 101, 192 138240      activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 139, 101, 192 576         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 139, 101, 192 0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 69, 50, 192)  0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 69, 50, 64)   12288       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 69, 50, 64)   192         conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 69, 50, 64)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 69, 50, 48)   9216        max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 69, 50, 96)   55296       activation_8[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 69, 50, 48)   144         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 69, 50, 96)   288         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 69, 50, 48)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 69, 50, 96)   0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","average_pooling2d (AveragePooli (None, 69, 50, 192)  0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 69, 50, 64)   12288       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 69, 50, 64)   76800       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 69, 50, 96)   82944       activation_9[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 69, 50, 32)   6144        average_pooling2d[0][0]          \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 69, 50, 64)   192         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 69, 50, 64)   192         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 69, 50, 96)   288         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 69, 50, 32)   96          conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 69, 50, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 69, 50, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 69, 50, 96)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 69, 50, 32)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","mixed0 (Concatenate)            (None, 69, 50, 256)  0           activation_5[0][0]               \n","                                                                 activation_7[0][0]               \n","                                                                 activation_10[0][0]              \n","                                                                 activation_11[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 69, 50, 64)   16384       mixed0[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 69, 50, 64)   192         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 69, 50, 64)   0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 69, 50, 48)   12288       mixed0[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 69, 50, 96)   55296       activation_15[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 69, 50, 48)   144         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 69, 50, 96)   288         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 69, 50, 48)   0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 69, 50, 96)   0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 69, 50, 256)  0           mixed0[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 69, 50, 64)   16384       mixed0[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 69, 50, 64)   76800       activation_13[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 69, 50, 96)   82944       activation_16[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 69, 50, 64)   16384       average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 69, 50, 64)   192         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 69, 50, 64)   192         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 69, 50, 96)   288         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 69, 50, 64)   192         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 69, 50, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 69, 50, 64)   0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 69, 50, 96)   0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 69, 50, 64)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","mixed1 (Concatenate)            (None, 69, 50, 288)  0           activation_12[0][0]              \n","                                                                 activation_14[0][0]              \n","                                                                 activation_17[0][0]              \n","                                                                 activation_18[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 69, 50, 64)   18432       mixed1[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 69, 50, 64)   192         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 69, 50, 64)   0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 69, 50, 48)   13824       mixed1[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 69, 50, 96)   55296       activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 69, 50, 48)   144         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 69, 50, 96)   288         conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 69, 50, 48)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 69, 50, 96)   0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_2 (AveragePoo (None, 69, 50, 288)  0           mixed1[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 69, 50, 64)   18432       mixed1[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 69, 50, 64)   76800       activation_20[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 69, 50, 96)   82944       activation_23[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 69, 50, 64)   18432       average_pooling2d_2[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 69, 50, 64)   192         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 69, 50, 64)   192         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 69, 50, 96)   288         conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 69, 50, 64)   192         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 69, 50, 64)   0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 69, 50, 64)   0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 69, 50, 96)   0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 69, 50, 64)   0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","mixed2 (Concatenate)            (None, 69, 50, 288)  0           activation_19[0][0]              \n","                                                                 activation_21[0][0]              \n","                                                                 activation_24[0][0]              \n","                                                                 activation_25[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 69, 50, 64)   18432       mixed2[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 69, 50, 64)   192         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 69, 50, 64)   0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 69, 50, 96)   55296       activation_27[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 69, 50, 96)   288         conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 69, 50, 96)   0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 34, 24, 384)  995328      mixed2[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 34, 24, 96)   82944       activation_28[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 34, 24, 384)  1152        conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 34, 24, 96)   288         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 34, 24, 384)  0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 34, 24, 96)   0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 34, 24, 288)  0           mixed2[0][0]                     \n","__________________________________________________________________________________________________\n","mixed3 (Concatenate)            (None, 34, 24, 768)  0           activation_26[0][0]              \n","                                                                 activation_29[0][0]              \n","                                                                 max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 34, 24, 128)  98304       mixed3[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 34, 24, 128)  384         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","activation_34 (Activation)      (None, 34, 24, 128)  0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 34, 24, 128)  114688      activation_34[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 34, 24, 128)  384         conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","activation_35 (Activation)      (None, 34, 24, 128)  0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 34, 24, 128)  98304       mixed3[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 34, 24, 128)  114688      activation_35[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 34, 24, 128)  384         conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 34, 24, 128)  384         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_31 (Activation)      (None, 34, 24, 128)  0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","activation_36 (Activation)      (None, 34, 24, 128)  0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 34, 24, 128)  114688      activation_31[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 34, 24, 128)  114688      activation_36[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 34, 24, 128)  384         conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 34, 24, 128)  384         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","activation_32 (Activation)      (None, 34, 24, 128)  0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","activation_37 (Activation)      (None, 34, 24, 128)  0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_3 (AveragePoo (None, 34, 24, 768)  0           mixed3[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 34, 24, 192)  147456      mixed3[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 34, 24, 192)  172032      activation_32[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 34, 24, 192)  172032      activation_37[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 34, 24, 192)  147456      average_pooling2d_3[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 34, 24, 192)  576         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 34, 24, 192)  576         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 34, 24, 192)  576         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 34, 24, 192)  576         conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","activation_30 (Activation)      (None, 34, 24, 192)  0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","activation_33 (Activation)      (None, 34, 24, 192)  0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","activation_38 (Activation)      (None, 34, 24, 192)  0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","activation_39 (Activation)      (None, 34, 24, 192)  0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","mixed4 (Concatenate)            (None, 34, 24, 768)  0           activation_30[0][0]              \n","                                                                 activation_33[0][0]              \n","                                                                 activation_38[0][0]              \n","                                                                 activation_39[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 34, 24, 160)  122880      mixed4[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_44 (BatchNo (None, 34, 24, 160)  480         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","activation_44 (Activation)      (None, 34, 24, 160)  0           batch_normalization_44[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 34, 24, 160)  179200      activation_44[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_45 (BatchNo (None, 34, 24, 160)  480         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","activation_45 (Activation)      (None, 34, 24, 160)  0           batch_normalization_45[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 34, 24, 160)  122880      mixed4[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 34, 24, 160)  179200      activation_45[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_41 (BatchNo (None, 34, 24, 160)  480         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_46 (BatchNo (None, 34, 24, 160)  480         conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","activation_41 (Activation)      (None, 34, 24, 160)  0           batch_normalization_41[0][0]     \n","__________________________________________________________________________________________________\n","activation_46 (Activation)      (None, 34, 24, 160)  0           batch_normalization_46[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 34, 24, 160)  179200      activation_41[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 34, 24, 160)  179200      activation_46[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_42 (BatchNo (None, 34, 24, 160)  480         conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_47 (BatchNo (None, 34, 24, 160)  480         conv2d_47[0][0]                  \n","__________________________________________________________________________________________________\n","activation_42 (Activation)      (None, 34, 24, 160)  0           batch_normalization_42[0][0]     \n","__________________________________________________________________________________________________\n","activation_47 (Activation)      (None, 34, 24, 160)  0           batch_normalization_47[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_4 (AveragePoo (None, 34, 24, 768)  0           mixed4[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 34, 24, 192)  147456      mixed4[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 34, 24, 192)  215040      activation_42[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 34, 24, 192)  215040      activation_47[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 34, 24, 192)  147456      average_pooling2d_4[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_40 (BatchNo (None, 34, 24, 192)  576         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_43 (BatchNo (None, 34, 24, 192)  576         conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_48 (BatchNo (None, 34, 24, 192)  576         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 34, 24, 192)  576         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","activation_40 (Activation)      (None, 34, 24, 192)  0           batch_normalization_40[0][0]     \n","__________________________________________________________________________________________________\n","activation_43 (Activation)      (None, 34, 24, 192)  0           batch_normalization_43[0][0]     \n","__________________________________________________________________________________________________\n","activation_48 (Activation)      (None, 34, 24, 192)  0           batch_normalization_48[0][0]     \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 34, 24, 192)  0           batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","mixed5 (Concatenate)            (None, 34, 24, 768)  0           activation_40[0][0]              \n","                                                                 activation_43[0][0]              \n","                                                                 activation_48[0][0]              \n","                                                                 activation_49[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 34, 24, 160)  122880      mixed5[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_54 (BatchNo (None, 34, 24, 160)  480         conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","activation_54 (Activation)      (None, 34, 24, 160)  0           batch_normalization_54[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 34, 24, 160)  179200      activation_54[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_55 (BatchNo (None, 34, 24, 160)  480         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_55 (Activation)      (None, 34, 24, 160)  0           batch_normalization_55[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 34, 24, 160)  122880      mixed5[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 34, 24, 160)  179200      activation_55[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_51 (BatchNo (None, 34, 24, 160)  480         conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_56 (BatchNo (None, 34, 24, 160)  480         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_51 (Activation)      (None, 34, 24, 160)  0           batch_normalization_51[0][0]     \n","__________________________________________________________________________________________________\n","activation_56 (Activation)      (None, 34, 24, 160)  0           batch_normalization_56[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 34, 24, 160)  179200      activation_51[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_57 (Conv2D)              (None, 34, 24, 160)  179200      activation_56[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_52 (BatchNo (None, 34, 24, 160)  480         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_57 (BatchNo (None, 34, 24, 160)  480         conv2d_57[0][0]                  \n","__________________________________________________________________________________________________\n","activation_52 (Activation)      (None, 34, 24, 160)  0           batch_normalization_52[0][0]     \n","__________________________________________________________________________________________________\n","activation_57 (Activation)      (None, 34, 24, 160)  0           batch_normalization_57[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_5 (AveragePoo (None, 34, 24, 768)  0           mixed5[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 34, 24, 192)  147456      mixed5[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 34, 24, 192)  215040      activation_52[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_58 (Conv2D)              (None, 34, 24, 192)  215040      activation_57[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_59 (Conv2D)              (None, 34, 24, 192)  147456      average_pooling2d_5[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_50 (BatchNo (None, 34, 24, 192)  576         conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_53 (BatchNo (None, 34, 24, 192)  576         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_58 (BatchNo (None, 34, 24, 192)  576         conv2d_58[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_59 (BatchNo (None, 34, 24, 192)  576         conv2d_59[0][0]                  \n","__________________________________________________________________________________________________\n","activation_50 (Activation)      (None, 34, 24, 192)  0           batch_normalization_50[0][0]     \n","__________________________________________________________________________________________________\n","activation_53 (Activation)      (None, 34, 24, 192)  0           batch_normalization_53[0][0]     \n","__________________________________________________________________________________________________\n","activation_58 (Activation)      (None, 34, 24, 192)  0           batch_normalization_58[0][0]     \n","__________________________________________________________________________________________________\n","activation_59 (Activation)      (None, 34, 24, 192)  0           batch_normalization_59[0][0]     \n","__________________________________________________________________________________________________\n","mixed6 (Concatenate)            (None, 34, 24, 768)  0           activation_50[0][0]              \n","                                                                 activation_53[0][0]              \n","                                                                 activation_58[0][0]              \n","                                                                 activation_59[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_64 (Conv2D)              (None, 34, 24, 192)  147456      mixed6[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_64 (BatchNo (None, 34, 24, 192)  576         conv2d_64[0][0]                  \n","__________________________________________________________________________________________________\n","activation_64 (Activation)      (None, 34, 24, 192)  0           batch_normalization_64[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_65 (Conv2D)              (None, 34, 24, 192)  258048      activation_64[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_65 (BatchNo (None, 34, 24, 192)  576         conv2d_65[0][0]                  \n","__________________________________________________________________________________________________\n","activation_65 (Activation)      (None, 34, 24, 192)  0           batch_normalization_65[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_61 (Conv2D)              (None, 34, 24, 192)  147456      mixed6[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_66 (Conv2D)              (None, 34, 24, 192)  258048      activation_65[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_61 (BatchNo (None, 34, 24, 192)  576         conv2d_61[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_66 (BatchNo (None, 34, 24, 192)  576         conv2d_66[0][0]                  \n","__________________________________________________________________________________________________\n","activation_61 (Activation)      (None, 34, 24, 192)  0           batch_normalization_61[0][0]     \n","__________________________________________________________________________________________________\n","activation_66 (Activation)      (None, 34, 24, 192)  0           batch_normalization_66[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_62 (Conv2D)              (None, 34, 24, 192)  258048      activation_61[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_67 (Conv2D)              (None, 34, 24, 192)  258048      activation_66[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_62 (BatchNo (None, 34, 24, 192)  576         conv2d_62[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_67 (BatchNo (None, 34, 24, 192)  576         conv2d_67[0][0]                  \n","__________________________________________________________________________________________________\n","activation_62 (Activation)      (None, 34, 24, 192)  0           batch_normalization_62[0][0]     \n","__________________________________________________________________________________________________\n","activation_67 (Activation)      (None, 34, 24, 192)  0           batch_normalization_67[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_6 (AveragePoo (None, 34, 24, 768)  0           mixed6[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_60 (Conv2D)              (None, 34, 24, 192)  147456      mixed6[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_63 (Conv2D)              (None, 34, 24, 192)  258048      activation_62[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_68 (Conv2D)              (None, 34, 24, 192)  258048      activation_67[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_69 (Conv2D)              (None, 34, 24, 192)  147456      average_pooling2d_6[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_60 (BatchNo (None, 34, 24, 192)  576         conv2d_60[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_63 (BatchNo (None, 34, 24, 192)  576         conv2d_63[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_68 (BatchNo (None, 34, 24, 192)  576         conv2d_68[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_69 (BatchNo (None, 34, 24, 192)  576         conv2d_69[0][0]                  \n","__________________________________________________________________________________________________\n","activation_60 (Activation)      (None, 34, 24, 192)  0           batch_normalization_60[0][0]     \n","__________________________________________________________________________________________________\n","activation_63 (Activation)      (None, 34, 24, 192)  0           batch_normalization_63[0][0]     \n","__________________________________________________________________________________________________\n","activation_68 (Activation)      (None, 34, 24, 192)  0           batch_normalization_68[0][0]     \n","__________________________________________________________________________________________________\n","activation_69 (Activation)      (None, 34, 24, 192)  0           batch_normalization_69[0][0]     \n","__________________________________________________________________________________________________\n","mixed7 (Concatenate)            (None, 34, 24, 768)  0           activation_60[0][0]              \n","                                                                 activation_63[0][0]              \n","                                                                 activation_68[0][0]              \n","                                                                 activation_69[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_72 (Conv2D)              (None, 34, 24, 192)  147456      mixed7[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization_72 (BatchNo (None, 34, 24, 192)  576         conv2d_72[0][0]                  \n","__________________________________________________________________________________________________\n","activation_72 (Activation)      (None, 34, 24, 192)  0           batch_normalization_72[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_73 (Conv2D)              (None, 34, 24, 192)  258048      activation_72[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_73 (BatchNo (None, 34, 24, 192)  576         conv2d_73[0][0]                  \n","__________________________________________________________________________________________________\n","activation_73 (Activation)      (None, 34, 24, 192)  0           batch_normalization_73[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_70 (Conv2D)              (None, 34, 24, 192)  147456      mixed7[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_74 (Conv2D)              (None, 34, 24, 192)  258048      activation_73[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_70 (BatchNo (None, 34, 24, 192)  576         conv2d_70[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_74 (BatchNo (None, 34, 24, 192)  576         conv2d_74[0][0]                  \n","__________________________________________________________________________________________________\n","activation_70 (Activation)      (None, 34, 24, 192)  0           batch_normalization_70[0][0]     \n","__________________________________________________________________________________________________\n","activation_74 (Activation)      (None, 34, 24, 192)  0           batch_normalization_74[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_71 (Conv2D)              (None, 16, 11, 320)  552960      activation_70[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_75 (Conv2D)              (None, 16, 11, 192)  331776      activation_74[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_71 (BatchNo (None, 16, 11, 320)  960         conv2d_71[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_75 (BatchNo (None, 16, 11, 192)  576         conv2d_75[0][0]                  \n","__________________________________________________________________________________________________\n","activation_71 (Activation)      (None, 16, 11, 320)  0           batch_normalization_71[0][0]     \n","__________________________________________________________________________________________________\n","activation_75 (Activation)      (None, 16, 11, 192)  0           batch_normalization_75[0][0]     \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 16, 11, 768)  0           mixed7[0][0]                     \n","__________________________________________________________________________________________________\n","mixed8 (Concatenate)            (None, 16, 11, 1280) 0           activation_71[0][0]              \n","                                                                 activation_75[0][0]              \n","                                                                 max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 225280)       0           mixed8[0][0]                     \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1024)         230687744   flatten[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 7)            7175        dropout[0][0]                    \n","==================================================================================================\n","Total params: 241,369,767\n","Trainable params: 230,694,919\n","Non-trainable params: 10,674,848\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WToVC0eNutGh","colab_type":"code","outputId":"4adbfde5-8da8-42d3-f451-e58ff3c265dd","executionInfo":{"status":"error","timestamp":1581979548017,"user_tz":-360,"elapsed":383647,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history = model.fit_generator(\n","            train_generator,\n","            validation_data = validation_generator,\n","            epochs = 500,\n","            callbacks=[cp_callback,AccuracyLimiterCallback],\n","            verbose = 1)\n","history.save ('/content/drive/My Drive/Thesis/test_2 cp/test_2.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/500\n","132/133 [============================>.] - ETA: 1s - loss: 4.0527 - acc: 0.1536Epoch 1/500\n","133/133 [==============================] - 251s 2s/step - loss: 4.0369 - acc: 0.1536 - val_loss: 1.9450 - val_acc: 0.1577\n","Epoch 2/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9621 - acc: 0.1476Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.9619 - acc: 0.1484 - val_loss: 1.9464 - val_acc: 0.1334\n","Epoch 3/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9518 - acc: 0.1460Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.9537 - acc: 0.1453 - val_loss: 1.9455 - val_acc: 0.1442\n","Epoch 4/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9540 - acc: 0.1440Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.9540 - acc: 0.1439 - val_loss: 1.9452 - val_acc: 0.1482\n","Epoch 5/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9516 - acc: 0.1481Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.9516 - acc: 0.1482 - val_loss: 1.9460 - val_acc: 0.1456\n","Epoch 6/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9499 - acc: 0.1479Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.9498 - acc: 0.1472 - val_loss: 1.9455 - val_acc: 0.1536\n","Epoch 7/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9450 - acc: 0.1543Epoch 1/500\n","133/133 [==============================] - 251s 2s/step - loss: 1.9450 - acc: 0.1543 - val_loss: 1.9459 - val_acc: 0.1550\n","Epoch 8/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9412 - acc: 0.1603Epoch 1/500\n","133/133 [==============================] - 258s 2s/step - loss: 1.9413 - acc: 0.1605 - val_loss: 1.9440 - val_acc: 0.1685\n","Epoch 9/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9412 - acc: 0.1617Epoch 1/500\n","133/133 [==============================] - 238s 2s/step - loss: 1.9413 - acc: 0.1614 - val_loss: 1.9377 - val_acc: 0.1860\n","Epoch 10/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9265 - acc: 0.1703Epoch 1/500\n","133/133 [==============================] - 264s 2s/step - loss: 1.9264 - acc: 0.1699 - val_loss: 1.9334 - val_acc: 0.2049\n","Epoch 11/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9145 - acc: 0.1755Epoch 1/500\n","133/133 [==============================] - 266s 2s/step - loss: 1.9138 - acc: 0.1763 - val_loss: 1.9085 - val_acc: 0.2062\n","Epoch 12/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.9010 - acc: 0.1853Epoch 1/500\n","133/133 [==============================] - 249s 2s/step - loss: 1.9010 - acc: 0.1851 - val_loss: 1.9166 - val_acc: 0.2116\n","Epoch 13/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8825 - acc: 0.1944Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.8819 - acc: 0.1950 - val_loss: 1.8926 - val_acc: 0.2224\n","Epoch 14/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8691 - acc: 0.1984Epoch 1/500\n","133/133 [==============================] - 254s 2s/step - loss: 1.8690 - acc: 0.1988 - val_loss: 1.8955 - val_acc: 0.1873\n","Epoch 15/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8639 - acc: 0.2008Epoch 1/500\n","133/133 [==============================] - 245s 2s/step - loss: 1.8640 - acc: 0.2012 - val_loss: 1.9023 - val_acc: 0.1873\n","Epoch 16/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8369 - acc: 0.2082Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.8373 - acc: 0.2083 - val_loss: 1.9219 - val_acc: 0.2008\n","Epoch 17/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8314 - acc: 0.2211Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.8315 - acc: 0.2201 - val_loss: 1.8970 - val_acc: 0.1846\n","Epoch 18/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8266 - acc: 0.2185Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.8261 - acc: 0.2192 - val_loss: 1.8753 - val_acc: 0.2102\n","Epoch 19/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.8180 - acc: 0.2194Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.8172 - acc: 0.2199 - val_loss: 1.9209 - val_acc: 0.1914\n","Epoch 20/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7997 - acc: 0.2263Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.8000 - acc: 0.2265 - val_loss: 1.9205 - val_acc: 0.1712\n","Epoch 21/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7944 - acc: 0.2418Epoch 1/500\n","133/133 [==============================] - 239s 2s/step - loss: 1.7948 - acc: 0.2424 - val_loss: 1.8936 - val_acc: 0.1846\n","Epoch 22/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7803 - acc: 0.2404Epoch 1/500\n","133/133 [==============================] - 264s 2s/step - loss: 1.7802 - acc: 0.2409 - val_loss: 1.8552 - val_acc: 0.2089\n","Epoch 23/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7966 - acc: 0.2428Epoch 1/500\n","133/133 [==============================] - 243s 2s/step - loss: 1.7966 - acc: 0.2424 - val_loss: 1.9522 - val_acc: 0.1563\n","Epoch 24/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7830 - acc: 0.2521Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.7842 - acc: 0.2521 - val_loss: 1.9303 - val_acc: 0.1644\n","Epoch 25/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7783 - acc: 0.2516Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.7790 - acc: 0.2518 - val_loss: 1.9228 - val_acc: 0.1779\n","Epoch 26/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7657 - acc: 0.2607Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.7649 - acc: 0.2613 - val_loss: 1.8625 - val_acc: 0.2210\n","Epoch 27/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7625 - acc: 0.2619Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.7633 - acc: 0.2622 - val_loss: 1.8548 - val_acc: 0.2116\n","Epoch 28/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7615 - acc: 0.2611Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.7605 - acc: 0.2618 - val_loss: 1.9028 - val_acc: 0.1954\n","Epoch 29/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7565 - acc: 0.2631Epoch 1/500\n","133/133 [==============================] - 250s 2s/step - loss: 1.7571 - acc: 0.2634 - val_loss: 1.9500 - val_acc: 0.1725\n","Epoch 30/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7442 - acc: 0.2771Epoch 1/500\n","133/133 [==============================] - 253s 2s/step - loss: 1.7440 - acc: 0.2774 - val_loss: 1.9063 - val_acc: 0.1806\n","Epoch 31/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7415 - acc: 0.2647Epoch 1/500\n","133/133 [==============================] - 240s 2s/step - loss: 1.7416 - acc: 0.2651 - val_loss: 1.8496 - val_acc: 0.2102\n","Epoch 32/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7460 - acc: 0.2767Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.7448 - acc: 0.2764 - val_loss: 1.9142 - val_acc: 0.2264\n","Epoch 33/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7424 - acc: 0.2812Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.7427 - acc: 0.2809 - val_loss: 1.9615 - val_acc: 0.1887\n","Epoch 34/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7422 - acc: 0.2802Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.7412 - acc: 0.2812 - val_loss: 1.8887 - val_acc: 0.2264\n","Epoch 35/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7243 - acc: 0.2819Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.7232 - acc: 0.2821 - val_loss: 1.8388 - val_acc: 0.2143\n","Epoch 36/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7240 - acc: 0.2685Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.7244 - acc: 0.2675 - val_loss: 1.9940 - val_acc: 0.1995\n","Epoch 37/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7107 - acc: 0.2902Epoch 1/500\n","133/133 [==============================] - 258s 2s/step - loss: 1.7106 - acc: 0.2897 - val_loss: 2.2570 - val_acc: 0.1631\n","Epoch 38/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7053 - acc: 0.2840Epoch 1/500\n","133/133 [==============================] - 240s 2s/step - loss: 1.7051 - acc: 0.2845 - val_loss: 2.0240 - val_acc: 0.1995\n","Epoch 39/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7184 - acc: 0.2764Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.7179 - acc: 0.2764 - val_loss: 1.9953 - val_acc: 0.1927\n","Epoch 40/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7038 - acc: 0.2962Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.7050 - acc: 0.2956 - val_loss: 2.0719 - val_acc: 0.2008\n","Epoch 41/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6917 - acc: 0.2879Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.6913 - acc: 0.2878 - val_loss: 2.0537 - val_acc: 0.1995\n","Epoch 42/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.7043 - acc: 0.2919Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.7032 - acc: 0.2933 - val_loss: 2.1193 - val_acc: 0.1954\n","Epoch 43/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6941 - acc: 0.2912Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6948 - acc: 0.2918 - val_loss: 2.0587 - val_acc: 0.2102\n","Epoch 44/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6917 - acc: 0.2974Epoch 1/500\n","133/133 [==============================] - 238s 2s/step - loss: 1.6912 - acc: 0.2982 - val_loss: 1.9341 - val_acc: 0.2237\n","Epoch 45/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6730 - acc: 0.2967Epoch 1/500\n","133/133 [==============================] - 247s 2s/step - loss: 1.6732 - acc: 0.2961 - val_loss: 2.2532 - val_acc: 0.1954\n","Epoch 46/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6787 - acc: 0.2991Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6784 - acc: 0.2989 - val_loss: 2.0891 - val_acc: 0.2237\n","Epoch 47/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6893 - acc: 0.2941Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.6887 - acc: 0.2954 - val_loss: 2.0908 - val_acc: 0.2089\n","Epoch 48/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6595 - acc: 0.2969Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6610 - acc: 0.2968 - val_loss: 2.0462 - val_acc: 0.2251\n","Epoch 49/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6618 - acc: 0.3060Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6625 - acc: 0.3058 - val_loss: 2.2213 - val_acc: 0.2075\n","Epoch 50/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6620 - acc: 0.2998Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6625 - acc: 0.2999 - val_loss: 2.1181 - val_acc: 0.2412\n","Epoch 51/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6544 - acc: 0.3110Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6558 - acc: 0.3103 - val_loss: 2.1275 - val_acc: 0.2278\n","Epoch 52/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6407 - acc: 0.2991Epoch 1/500\n","133/133 [==============================] - 250s 2s/step - loss: 1.6411 - acc: 0.2989 - val_loss: 2.2892 - val_acc: 0.1954\n","Epoch 53/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6512 - acc: 0.3150Epoch 1/500\n","133/133 [==============================] - 247s 2s/step - loss: 1.6495 - acc: 0.3160 - val_loss: 2.4282 - val_acc: 0.2049\n","Epoch 54/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6473 - acc: 0.2995Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6468 - acc: 0.3004 - val_loss: 2.0785 - val_acc: 0.2520\n","Epoch 55/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6480 - acc: 0.3105Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6485 - acc: 0.3096 - val_loss: 2.1576 - val_acc: 0.2332\n","Epoch 56/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6493 - acc: 0.3119Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6491 - acc: 0.3115 - val_loss: 2.2443 - val_acc: 0.2170\n","Epoch 57/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6621 - acc: 0.2998Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6617 - acc: 0.2999 - val_loss: 2.1728 - val_acc: 0.2156\n","Epoch 58/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6459 - acc: 0.3143Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.6479 - acc: 0.3134 - val_loss: 2.3379 - val_acc: 0.2129\n","Epoch 59/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6193 - acc: 0.3210Epoch 1/500\n","133/133 [==============================] - 243s 2s/step - loss: 1.6202 - acc: 0.3212 - val_loss: 2.3247 - val_acc: 0.2210\n","Epoch 60/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6302 - acc: 0.3069Epoch 1/500\n","133/133 [==============================] - 247s 2s/step - loss: 1.6303 - acc: 0.3065 - val_loss: 2.3698 - val_acc: 0.2089\n","Epoch 61/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6619 - acc: 0.3177Epoch 1/500\n","133/133 [==============================] - 238s 2s/step - loss: 1.6618 - acc: 0.3183 - val_loss: 2.3671 - val_acc: 0.2116\n","Epoch 62/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6332 - acc: 0.3034Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6333 - acc: 0.3034 - val_loss: 2.2987 - val_acc: 0.2102\n","Epoch 63/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6416 - acc: 0.3150Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6404 - acc: 0.3150 - val_loss: 2.2318 - val_acc: 0.2278\n","Epoch 64/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6248 - acc: 0.3181Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6252 - acc: 0.3172 - val_loss: 2.2747 - val_acc: 0.2291\n","Epoch 65/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6345 - acc: 0.3167Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.6341 - acc: 0.3164 - val_loss: 2.2498 - val_acc: 0.2210\n","Epoch 66/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6392 - acc: 0.3170Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6396 - acc: 0.3167 - val_loss: 2.2796 - val_acc: 0.2318\n","Epoch 67/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6247 - acc: 0.3227Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.6245 - acc: 0.3221 - val_loss: 2.2878 - val_acc: 0.2305\n","Epoch 68/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6347 - acc: 0.3067Epoch 1/500\n","133/133 [==============================] - 247s 2s/step - loss: 1.6354 - acc: 0.3070 - val_loss: 2.0560 - val_acc: 0.2493\n","Epoch 69/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6295 - acc: 0.3275Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6292 - acc: 0.3278 - val_loss: 2.2593 - val_acc: 0.2332\n","Epoch 70/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6637 - acc: 0.3203Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6625 - acc: 0.3212 - val_loss: 2.4071 - val_acc: 0.2547\n","Epoch 71/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6304 - acc: 0.3153Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6308 - acc: 0.3150 - val_loss: 2.3992 - val_acc: 0.2439\n","Epoch 72/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6260 - acc: 0.3239Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6265 - acc: 0.3238 - val_loss: 2.3636 - val_acc: 0.2291\n","Epoch 73/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6441 - acc: 0.3108Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6449 - acc: 0.3105 - val_loss: 2.5179 - val_acc: 0.2385\n","Epoch 74/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6289 - acc: 0.3181Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6281 - acc: 0.3183 - val_loss: 2.1835 - val_acc: 0.2682\n","Epoch 75/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6448 - acc: 0.3177Epoch 1/500\n","133/133 [==============================] - 246s 2s/step - loss: 1.6439 - acc: 0.3179 - val_loss: 2.2542 - val_acc: 0.2682\n","Epoch 76/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6330 - acc: 0.3208Epoch 1/500\n","133/133 [==============================] - 242s 2s/step - loss: 1.6346 - acc: 0.3200 - val_loss: 2.1057 - val_acc: 0.2358\n","Epoch 77/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6200 - acc: 0.3160Epoch 1/500\n","133/133 [==============================] - 240s 2s/step - loss: 1.6202 - acc: 0.3153 - val_loss: 2.0967 - val_acc: 0.2628\n","Epoch 78/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6065 - acc: 0.3334Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6072 - acc: 0.3325 - val_loss: 2.1821 - val_acc: 0.2439\n","Epoch 79/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6388 - acc: 0.3072Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6384 - acc: 0.3077 - val_loss: 2.2102 - val_acc: 0.2642\n","Epoch 80/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6207 - acc: 0.3177Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6214 - acc: 0.3172 - val_loss: 2.1567 - val_acc: 0.2615\n","Epoch 81/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6212 - acc: 0.3291Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6228 - acc: 0.3278 - val_loss: 2.1931 - val_acc: 0.2628\n","Epoch 82/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6338 - acc: 0.3201Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.6343 - acc: 0.3202 - val_loss: 2.2334 - val_acc: 0.2615\n","Epoch 83/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6228 - acc: 0.3186Epoch 1/500\n","133/133 [==============================] - 254s 2s/step - loss: 1.6218 - acc: 0.3183 - val_loss: 2.3140 - val_acc: 0.2682\n","Epoch 84/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6044 - acc: 0.3322Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6036 - acc: 0.3323 - val_loss: 2.2910 - val_acc: 0.2520\n","Epoch 85/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6168 - acc: 0.3227Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6178 - acc: 0.3219 - val_loss: 2.3045 - val_acc: 0.2453\n","Epoch 86/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6174 - acc: 0.3210Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6168 - acc: 0.3209 - val_loss: 2.1953 - val_acc: 0.2655\n","Epoch 87/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6107 - acc: 0.3296Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6109 - acc: 0.3297 - val_loss: 2.3849 - val_acc: 0.2453\n","Epoch 88/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6338 - acc: 0.3150Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6346 - acc: 0.3150 - val_loss: 2.2433 - val_acc: 0.2574\n","Epoch 89/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6063 - acc: 0.3296Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6070 - acc: 0.3295 - val_loss: 2.2553 - val_acc: 0.2736\n","Epoch 90/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6286 - acc: 0.3222Epoch 1/500\n","133/133 [==============================] - 252s 2s/step - loss: 1.6300 - acc: 0.3212 - val_loss: 2.0217 - val_acc: 0.2911\n","Epoch 91/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6161 - acc: 0.3289Epoch 1/500\n","133/133 [==============================] - 242s 2s/step - loss: 1.6172 - acc: 0.3280 - val_loss: 2.2545 - val_acc: 0.2763\n","Epoch 92/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6117 - acc: 0.3277Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6115 - acc: 0.3273 - val_loss: 2.3443 - val_acc: 0.2601\n","Epoch 93/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6187 - acc: 0.3289Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6178 - acc: 0.3288 - val_loss: 2.2856 - val_acc: 0.2695\n","Epoch 94/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6017 - acc: 0.3306Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6024 - acc: 0.3311 - val_loss: 2.1084 - val_acc: 0.2871\n","Epoch 95/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6195 - acc: 0.3260Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6193 - acc: 0.3276 - val_loss: 2.1075 - val_acc: 0.2992\n","Epoch 96/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6175 - acc: 0.3260Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6172 - acc: 0.3262 - val_loss: 2.2960 - val_acc: 0.2561\n","Epoch 97/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6238 - acc: 0.3294Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6238 - acc: 0.3292 - val_loss: 2.2692 - val_acc: 0.2803\n","Epoch 98/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6275 - acc: 0.3282Epoch 1/500\n","133/133 [==============================] - 247s 2s/step - loss: 1.6260 - acc: 0.3283 - val_loss: 2.3508 - val_acc: 0.2830\n","Epoch 99/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6146 - acc: 0.3327Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6146 - acc: 0.3321 - val_loss: 2.4151 - val_acc: 0.2668\n","Epoch 100/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6336 - acc: 0.3236Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6317 - acc: 0.3236 - val_loss: 2.2522 - val_acc: 0.2871\n","Epoch 101/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6314 - acc: 0.3246Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.6291 - acc: 0.3257 - val_loss: 2.2390 - val_acc: 0.2790\n","Epoch 102/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6218 - acc: 0.3198Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.6206 - acc: 0.3200 - val_loss: 2.3654 - val_acc: 0.2709\n","Epoch 103/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6109 - acc: 0.3248Epoch 1/500\n","133/133 [==============================] - 229s 2s/step - loss: 1.6111 - acc: 0.3247 - val_loss: 2.3472 - val_acc: 0.2709\n","Epoch 104/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6017 - acc: 0.3348Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6020 - acc: 0.3337 - val_loss: 2.4769 - val_acc: 0.2466\n","Epoch 105/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6201 - acc: 0.3232Epoch 1/500\n","133/133 [==============================] - 244s 2s/step - loss: 1.6180 - acc: 0.3245 - val_loss: 2.3618 - val_acc: 0.2898\n","Epoch 106/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6102 - acc: 0.3338Epoch 1/500\n","133/133 [==============================] - 248s 2s/step - loss: 1.6123 - acc: 0.3337 - val_loss: 2.4712 - val_acc: 0.2628\n","Epoch 107/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6204 - acc: 0.3272Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6208 - acc: 0.3269 - val_loss: 2.3710 - val_acc: 0.2561\n","Epoch 108/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6034 - acc: 0.3306Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6023 - acc: 0.3321 - val_loss: 2.5093 - val_acc: 0.2507\n","Epoch 109/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.5912 - acc: 0.3396Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.5929 - acc: 0.3401 - val_loss: 2.3775 - val_acc: 0.2709\n","Epoch 110/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6191 - acc: 0.3229Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6194 - acc: 0.3219 - val_loss: 2.2010 - val_acc: 0.2736\n","Epoch 111/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6120 - acc: 0.3334Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6131 - acc: 0.3328 - val_loss: 2.3042 - val_acc: 0.2857\n","Epoch 112/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6098 - acc: 0.3313Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.6091 - acc: 0.3316 - val_loss: 2.3228 - val_acc: 0.2682\n","Epoch 113/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6217 - acc: 0.3344Epoch 1/500\n","133/133 [==============================] - 250s 2s/step - loss: 1.6214 - acc: 0.3349 - val_loss: 2.4366 - val_acc: 0.2588\n","Epoch 114/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6398 - acc: 0.3329Epoch 1/500\n","133/133 [==============================] - 246s 2s/step - loss: 1.6400 - acc: 0.3330 - val_loss: 2.3372 - val_acc: 0.2628\n","Epoch 115/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6152 - acc: 0.3308Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.6152 - acc: 0.3309 - val_loss: 2.3312 - val_acc: 0.2642\n","Epoch 116/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6254 - acc: 0.3310Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.6247 - acc: 0.3309 - val_loss: 2.4179 - val_acc: 0.2615\n","Epoch 117/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6153 - acc: 0.3298Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6152 - acc: 0.3297 - val_loss: 2.2711 - val_acc: 0.3154\n","Epoch 118/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6210 - acc: 0.3260Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6189 - acc: 0.3264 - val_loss: 2.1866 - val_acc: 0.3032\n","Epoch 119/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6268 - acc: 0.3279Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6279 - acc: 0.3278 - val_loss: 2.5264 - val_acc: 0.2817\n","Epoch 120/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6089 - acc: 0.3279Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.6080 - acc: 0.3278 - val_loss: 2.2513 - val_acc: 0.2803\n","Epoch 121/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6075 - acc: 0.3372Epoch 1/500\n","133/133 [==============================] - 238s 2s/step - loss: 1.6077 - acc: 0.3380 - val_loss: 2.1999 - val_acc: 0.3100\n","Epoch 122/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6198 - acc: 0.3239Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6197 - acc: 0.3236 - val_loss: 2.1767 - val_acc: 0.3032\n","Epoch 123/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6323 - acc: 0.3267Epoch 1/500\n","133/133 [==============================] - 243s 2s/step - loss: 1.6309 - acc: 0.3276 - val_loss: 2.2581 - val_acc: 0.2830\n","Epoch 124/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6127 - acc: 0.3263Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6125 - acc: 0.3264 - val_loss: 2.2019 - val_acc: 0.2898\n","Epoch 125/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6025 - acc: 0.3365Epoch 1/500\n","133/133 [==============================] - 234s 2s/step - loss: 1.6023 - acc: 0.3359 - val_loss: 2.1449 - val_acc: 0.2938\n","Epoch 126/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.5980 - acc: 0.3377Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.5990 - acc: 0.3375 - val_loss: 2.2463 - val_acc: 0.2790\n","Epoch 127/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.5849 - acc: 0.3484Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.5870 - acc: 0.3467 - val_loss: 2.4203 - val_acc: 0.2978\n","Epoch 128/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6118 - acc: 0.3251Epoch 1/500\n","133/133 [==============================] - 248s 2s/step - loss: 1.6112 - acc: 0.3259 - val_loss: 2.3428 - val_acc: 0.2561\n","Epoch 129/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6206 - acc: 0.3272Epoch 1/500\n","133/133 [==============================] - 242s 2s/step - loss: 1.6190 - acc: 0.3271 - val_loss: 2.0651 - val_acc: 0.2938\n","Epoch 130/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6171 - acc: 0.3501Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6172 - acc: 0.3496 - val_loss: 2.2729 - val_acc: 0.2951\n","Epoch 131/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6023 - acc: 0.3377Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6029 - acc: 0.3370 - val_loss: 2.2062 - val_acc: 0.2938\n","Epoch 132/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6105 - acc: 0.3356Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6135 - acc: 0.3342 - val_loss: 2.2878 - val_acc: 0.2790\n","Epoch 133/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6102 - acc: 0.3286Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6112 - acc: 0.3280 - val_loss: 2.2423 - val_acc: 0.2803\n","Epoch 134/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6019 - acc: 0.3282Epoch 1/500\n","133/133 [==============================] - 233s 2s/step - loss: 1.6019 - acc: 0.3266 - val_loss: 2.1919 - val_acc: 0.2871\n","Epoch 135/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6172 - acc: 0.3196Epoch 1/500\n","133/133 [==============================] - 231s 2s/step - loss: 1.6160 - acc: 0.3205 - val_loss: 2.2007 - val_acc: 0.2925\n","Epoch 136/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6095 - acc: 0.3346Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6093 - acc: 0.3344 - val_loss: 2.2519 - val_acc: 0.2709\n","Epoch 137/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6124 - acc: 0.3255Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6122 - acc: 0.3250 - val_loss: 2.3354 - val_acc: 0.2803\n","Epoch 138/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6063 - acc: 0.3377Epoch 1/500\n","133/133 [==============================] - 245s 2s/step - loss: 1.6055 - acc: 0.3375 - val_loss: 2.2097 - val_acc: 0.2925\n","Epoch 139/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6214 - acc: 0.3260Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6206 - acc: 0.3278 - val_loss: 2.3692 - val_acc: 0.2776\n","Epoch 140/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6096 - acc: 0.3320Epoch 1/500\n","133/133 [==============================] - 239s 2s/step - loss: 1.6087 - acc: 0.3325 - val_loss: 2.3668 - val_acc: 0.2803\n","Epoch 141/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6046 - acc: 0.3441Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6036 - acc: 0.3451 - val_loss: 2.3030 - val_acc: 0.2817\n","Epoch 142/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6007 - acc: 0.3360Epoch 1/500\n","133/133 [==============================] - 240s 2s/step - loss: 1.6001 - acc: 0.3361 - val_loss: 2.2264 - val_acc: 0.2817\n","Epoch 143/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6050 - acc: 0.3265Epoch 1/500\n","133/133 [==============================] - 237s 2s/step - loss: 1.6060 - acc: 0.3254 - val_loss: 2.2322 - val_acc: 0.2884\n","Epoch 144/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6154 - acc: 0.3329Epoch 1/500\n","133/133 [==============================] - 232s 2s/step - loss: 1.6159 - acc: 0.3318 - val_loss: 2.2253 - val_acc: 0.2938\n","Epoch 145/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6155 - acc: 0.3432Epoch 1/500\n","133/133 [==============================] - 235s 2s/step - loss: 1.6169 - acc: 0.3432 - val_loss: 2.2449 - val_acc: 0.2817\n","Epoch 146/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6228 - acc: 0.3310Epoch 1/500\n","133/133 [==============================] - 236s 2s/step - loss: 1.6234 - acc: 0.3302 - val_loss: 2.2496 - val_acc: 0.2978\n","Epoch 147/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6235 - acc: 0.3348Epoch 1/500\n","133/133 [==============================] - 241s 2s/step - loss: 1.6236 - acc: 0.3354 - val_loss: 2.2118 - val_acc: 0.2682\n","Epoch 148/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6065 - acc: 0.3301Epoch 1/500\n","133/133 [==============================] - 239s 2s/step - loss: 1.6058 - acc: 0.3297 - val_loss: 2.3216 - val_acc: 0.2965\n","Epoch 149/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6297 - acc: 0.3310Epoch 1/500\n","133/133 [==============================] - 238s 2s/step - loss: 1.6288 - acc: 0.3309 - val_loss: 2.1962 - val_acc: 0.2992\n","Epoch 150/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.5892 - acc: 0.3432Epoch 1/500\n","133/133 [==============================] - 230s 2s/step - loss: 1.5906 - acc: 0.3427 - val_loss: 2.1430 - val_acc: 0.3046\n","Epoch 151/500\n","132/133 [============================>.] - ETA: 1s - loss: 1.6022 - acc: 0.3351Epoch 1/500\n","133/133 [==============================] - 243s 2s/step - loss: 1.6003 - acc: 0.3359 - val_loss: 2.2287 - val_acc: 0.2736\n","Epoch 152/500\n","102/133 [======================>.......] - ETA: 46s - loss: 1.5945 - acc: 0.3492"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OBisk7Z6PR7j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"outputId":"fde70686-daa2-4aff-b41d-1cd19bdb154c","executionInfo":{"status":"error","timestamp":1582045150068,"user_tz":-360,"elapsed":879,"user":{"displayName":"Ashik Mostofa Tonmoy","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBwWBKaJ4EukXHcpnNGNuhb9_Zfn_Vg6aZ8JjyZXw=s64","userId":"00425423406010605528"}}},"source":["model.save ('/content/drive/My Drive/Thesis/test_2 cp/test_2.h5')"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7d7f21d0e7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Thesis/test_2 cp/test_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"nWxDvtFCWpIT","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","plt.figure()"],"execution_count":0,"outputs":[]}]}
